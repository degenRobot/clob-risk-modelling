{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Oracle Risk Analysis: High-Frequency Price Movements\n\nThis notebook analyzes 1-second spot price data to understand oracle-related risks and inform maximum leverage settings.\n\n## Executive Summary\n\nOracle risks directly impact:\n- **Maximum Leverage**: Extreme price moves can liquidate positions before they can react\n- **Liquidation Engine**: Must handle sudden price gaps gracefully\n- **Multi-Oracle Aggregation**: Understanding price deviation helps set deviation thresholds\n\nBy analyzing 1-second spot kline data (which closely tracks mark price), we can quantify:\n- Distribution of price movements at various time scales\n- Extreme tail events (99th, 99.9th, 99.99th percentiles)\n- Maximum observed price gaps\n- Safe leverage levels based on historical data\n\n**Note**: We use Binance Spot API for 1s data as Futures API doesn't support 1s intervals. Spot price closely tracks futures mark price for major pairs like BTC."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Essential imports\nimport sys\nsys.path.append('../src')\n\nfrom risk_model.chart_config import setup_chart_style, COLORS\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nsetup_chart_style()\n\n# Configuration\nSYMBOL = \"BTCUSDT\"\nDATA_DIR = Path(\"../data/spot_klines\")\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# Memory info\nimport os\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## 1. Load Data\n\nData is downloaded separately using the `scripts/download_klines.py` script.\n\n**To download data:**\n```bash\n# Test with 7 days\npoetry run python scripts/download_klines.py --symbol BTCUSDT --days 7\n\n# Full year\npoetry run python scripts/download_klines.py --symbol BTCUSDT --days 365\n\n# Multiple pairs\npoetry run python scripts/download_klines.py --symbol BTCUSDT ETHUSDT SOLUSDT --days 365\n```\n\nThe script has resume capability - just run the same command to continue an interrupted download."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Load data from CSV (downloaded by scripts/download_klines.py)\ncsv_path = DATA_DIR / f\"{SYMBOL}_1s.csv\"\n\nif csv_path.exists():\n    # Get file size\n    file_size_mb = csv_path.stat().st_size / (1024 * 1024)\n    print(f\"Loading {csv_path.name} ({file_size_mb:.1f} MB)...\")\n    \n    # Use efficient dtypes to reduce memory\n    dtype_spec = {\n        'open': 'float32',\n        'high': 'float32', \n        'low': 'float32',\n        'close': 'float32',\n        'volume': 'float32'\n    }\n    \n    df = pd.read_csv(\n        csv_path, \n        parse_dates=['timestamp'],\n        dtype=dtype_spec\n    )\n    \n    # Memory usage\n    mem_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n    print(f\"Loaded {len(df):,} rows ({mem_mb:.1f} MB in memory)\")\n    print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n    print(f\"Duration: {(df['timestamp'].max() - df['timestamp'].min()).days} days\")\nelse:\n    print(f\"No data file found at {csv_path}\")\n    print(f\"\\nRun the downloader first:\")\n    print(f\"  poetry run python scripts/download_klines.py --symbol {SYMBOL} --days 7\")\n    raise FileNotFoundError(f\"Data file not found: {csv_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data overview\n",
    "print(f\"Data Range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Total Duration: {(df['timestamp'].max() - df['timestamp'].min()).days} days\")\n",
    "print(f\"Total Rows: {len(df):,}\")\n",
    "print(f\"\\nData Preview:\")\n",
    "display(df.head(10))\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Price Movement Calculations\n",
    "\n",
    "We calculate price changes at multiple time scales:\n",
    "- **1 second**: Tick-by-tick movements\n",
    "- **5 seconds**: Short-term volatility\n",
    "- **30 seconds**: Trading decision timeframe\n",
    "- **1 minute**: Standard candle interval\n",
    "- **5 minutes**: Liquidation reaction window\n",
    "\n",
    "Both absolute and percentage changes are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate price changes at different time scales\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Use close price for analysis\n",
    "price = df['close']\n",
    "\n",
    "# Calculate returns at different windows\n",
    "windows = {\n",
    "    '1s': 1,\n",
    "    '5s': 5,\n",
    "    '30s': 30,\n",
    "    '1m': 60,\n",
    "    '5m': 300\n",
    "}\n",
    "\n",
    "returns_df = pd.DataFrame(index=df.index)\n",
    "returns_df['price'] = price\n",
    "\n",
    "for name, periods in windows.items():\n",
    "    returns_df[f'return_{name}'] = price.pct_change(periods=periods) * 100  # In percent\n",
    "    returns_df[f'abs_change_{name}'] = price.diff(periods=periods).abs()\n",
    "\n",
    "# Also calculate high-low range within each second (intra-candle volatility)\n",
    "returns_df['intra_range_pct'] = ((df['high'] - df['low']) / df['close']) * 100\n",
    "\n",
    "print(\"Calculated return statistics:\")\n",
    "display(returns_df[[f'return_{w}' for w in windows.keys()]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Distribution Analysis\n",
    "\n",
    "We visualize the distribution of price movements to understand:\n",
    "- Central tendency and spread\n",
    "- Fat tails (extreme events)\n",
    "- Asymmetry (more down moves vs up moves?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Plot histograms of returns at different time scales\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor idx, (name, periods) in enumerate(windows.items()):\n    ax = axes[idx]\n    data = returns_df[f'return_{name}'].dropna()\n    \n    # Remove extreme outliers for visualization (keep for statistics)\n    q99 = data.abs().quantile(0.99)\n    plot_data = data[data.abs() <= q99 * 2]\n    \n    ax.hist(plot_data, bins=100, alpha=0.7, color=COLORS['primary'], edgecolor='none')\n    ax.axvline(x=0, color=COLORS['danger'], linestyle='--', alpha=0.7)\n    \n    # Add statistics annotation\n    stats_text = f\"Mean: {data.mean():.4f}%\\nStd: {data.std():.4f}%\\n99th: {data.abs().quantile(0.99):.4f}%\"\n    ax.text(0.95, 0.95, stats_text, transform=ax.transAxes, fontsize=9,\n            verticalalignment='top', horizontalalignment='right',\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n    \n    ax.set_title(f'{name} Returns Distribution')\n    ax.set_xlabel('Return (%)')\n    ax.set_ylabel('Frequency')\n    ax.grid(True, alpha=0.3)\n\n# Intra-candle volatility\nax = axes[5]\nintra_data = returns_df['intra_range_pct'].dropna()\nq99_intra = intra_data.quantile(0.99)\nax.hist(intra_data[intra_data <= q99_intra * 2], bins=100, alpha=0.7, \n        color=COLORS['warning'], edgecolor='none')\nax.set_title('Intra-Second Range Distribution')\nax.set_xlabel('High-Low Range (%)')\nax.set_ylabel('Frequency')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(DATA_DIR / 'returns_distribution.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Extreme Movement Analysis\n",
    "\n",
    "For setting leverage limits, we need to understand tail risks:\n",
    "- **99th percentile**: 1 in 100 events\n",
    "- **99.9th percentile**: 1 in 1,000 events (~3 per day at 1s resolution)\n",
    "- **99.99th percentile**: 1 in 10,000 events\n",
    "- **Maximum observed**: Worst case in historical data\n",
    "\n",
    "A position can be liquidated if price moves against it by `1 / leverage`. For example:\n",
    "- 10x leverage: liquidated at 10% adverse move\n",
    "- 20x leverage: liquidated at 5% adverse move\n",
    "- 50x leverage: liquidated at 2% adverse move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentile statistics for all time windows\n",
    "percentiles = [0.9, 0.95, 0.99, 0.999, 0.9999]\n",
    "percentile_stats = {}\n",
    "\n",
    "for name in windows.keys():\n",
    "    col = f'return_{name}'\n",
    "    data = returns_df[col].dropna().abs()  # Absolute returns\n",
    "    \n",
    "    stats = {\n",
    "        'mean': data.mean(),\n",
    "        'std': data.std(),\n",
    "        'max': data.max(),\n",
    "        'count': len(data)\n",
    "    }\n",
    "    \n",
    "    for p in percentiles:\n",
    "        stats[f'p{int(p*100)}' if p < 1 else f'p{p*100:.2f}'] = data.quantile(p)\n",
    "    \n",
    "    percentile_stats[name] = stats\n",
    "\n",
    "stats_df = pd.DataFrame(percentile_stats).T\n",
    "print(\"Extreme Movement Statistics (Absolute % Returns):\")\n",
    "display(stats_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize extreme percentiles across time windows\nfig, ax = plt.subplots(figsize=(12, 6))\n\nwindow_names = list(windows.keys())\nx = np.arange(len(window_names))\nwidth = 0.15\n\ncolors_pct = [COLORS['primary'], COLORS['secondary'], COLORS['warning'], \n              COLORS['danger'], '#8B0000']\n\nfor i, p in enumerate(percentiles):\n    label = f\"{p*100:.2f}%\" if p > 0.99 else f\"{int(p*100)}%\"\n    col = f'p{int(p*100)}' if p < 1 else f'p{p*100:.2f}'\n    values = [percentile_stats[w][col] for w in window_names]\n    ax.bar(x + i*width, values, width, label=label, color=colors_pct[i], alpha=0.8)\n\nax.set_xlabel('Time Window')\nax.set_ylabel('Absolute Return (%)')\nax.set_title('Extreme Price Movements by Percentile and Time Window')\nax.set_xticks(x + width * 2)\nax.set_xticklabels(window_names)\nax.legend(title='Percentile')\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig(DATA_DIR / 'extreme_movements.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Maximum Drawdown Analysis\n",
    "\n",
    "Beyond point-to-point returns, we analyze rolling maximum drawdowns:\n",
    "- Maximum adverse move within a rolling window\n",
    "- Identifies sustained adverse price moves\n",
    "- Critical for understanding liquidation risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling maximum drawdown for different windows\n",
    "def calculate_max_drawdown(prices, window):\n",
    "    \"\"\"Calculate rolling maximum drawdown over a window.\"\"\"\n",
    "    rolling_max = prices.rolling(window=window, min_periods=1).max()\n",
    "    drawdown = (prices - rolling_max) / rolling_max * 100\n",
    "    return drawdown\n",
    "\n",
    "drawdown_windows = {\n",
    "    '1m': 60,\n",
    "    '5m': 300,\n",
    "    '15m': 900,\n",
    "    '1h': 3600\n",
    "}\n",
    "\n",
    "drawdowns = {}\n",
    "for name, window in drawdown_windows.items():\n",
    "    dd = calculate_max_drawdown(price, window)\n",
    "    drawdowns[name] = dd\n",
    "    \n",
    "# Statistics\n",
    "dd_stats = {}\n",
    "for name, dd in drawdowns.items():\n",
    "    dd_clean = dd.dropna()\n",
    "    dd_stats[name] = {\n",
    "        'min_dd': dd_clean.min(),  # Worst drawdown (most negative)\n",
    "        'p1': dd_clean.quantile(0.01),\n",
    "        'p5': dd_clean.quantile(0.05),\n",
    "        'mean': dd_clean.mean(),\n",
    "        'p95': dd_clean.quantile(0.95),\n",
    "        'p99': dd_clean.quantile(0.99)\n",
    "    }\n",
    "\n",
    "dd_df = pd.DataFrame(dd_stats).T\n",
    "print(\"Rolling Maximum Drawdown Statistics (%):\")\n",
    "display(dd_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# Plot drawdown distribution for key windows\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor idx, (name, dd) in enumerate(drawdowns.items()):\n    ax = axes[idx]\n    dd_clean = dd.dropna()\n    \n    # Focus on negative drawdowns\n    negative_dd = dd_clean[dd_clean < 0]\n    \n    ax.hist(negative_dd, bins=100, alpha=0.7, color=COLORS['danger'], edgecolor='none')\n    \n    # Add percentile lines\n    p1 = negative_dd.quantile(0.01)\n    p5 = negative_dd.quantile(0.05)\n    ax.axvline(x=p1, color='black', linestyle='--', label=f'1st pct: {p1:.2f}%')\n    ax.axvline(x=p5, color='gray', linestyle='--', label=f'5th pct: {p5:.2f}%')\n    \n    ax.set_title(f'{name} Rolling Drawdown Distribution')\n    ax.set_xlabel('Drawdown (%)')\n    ax.set_ylabel('Frequency')\n    ax.legend(loc='upper left')\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(DATA_DIR / 'drawdown_distribution.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Leverage Recommendations\n",
    "\n",
    "Based on observed price movements, we can recommend safe leverage levels.\n",
    "\n",
    "**Methodology:**\n",
    "- A position with leverage L is liquidated if price moves against it by 1/L (minus maintenance margin)\n",
    "- We want liquidations to be rare events (not triggered by normal volatility)\n",
    "- Target: Liquidation should not occur more often than X% of the time\n",
    "\n",
    "**Assumptions:**\n",
    "- Maintenance margin buffer: ~1% (varies by exchange)\n",
    "- Liquidation engine reaction time: 5 seconds to 5 minutes depending on network conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate safe leverage based on observed volatility\n",
    "def calculate_safe_leverage(max_move_pct, safety_buffer=1.0):\n",
    "    \"\"\"Calculate safe leverage given a maximum expected price move.\n",
    "    \n",
    "    Args:\n",
    "        max_move_pct: Maximum expected price move in percent\n",
    "        safety_buffer: Additional buffer in percent (default 1%)\n",
    "    \n",
    "    Returns:\n",
    "        Maximum safe leverage\n",
    "    \"\"\"\n",
    "    total_move = max_move_pct + safety_buffer\n",
    "    if total_move <= 0:\n",
    "        return float('inf')\n",
    "    return 100 / total_move\n",
    "\n",
    "# Calculate for different time windows and percentiles\n",
    "leverage_recs = {}\n",
    "\n",
    "for window in ['1m', '5m']:\n",
    "    if window in drawdown_windows:\n",
    "        dd = drawdowns[window].dropna()\n",
    "        worst_dd = abs(dd.min())\n",
    "        p1_dd = abs(dd.quantile(0.01))\n",
    "        p5_dd = abs(dd.quantile(0.05))\n",
    "        \n",
    "        leverage_recs[window] = {\n",
    "            'worst_drawdown_pct': worst_dd,\n",
    "            'p1_drawdown_pct': p1_dd,\n",
    "            'p5_drawdown_pct': p5_dd,\n",
    "            'max_leverage_conservative': calculate_safe_leverage(worst_dd, safety_buffer=2.0),\n",
    "            'max_leverage_moderate': calculate_safe_leverage(p1_dd, safety_buffer=1.5),\n",
    "            'max_leverage_aggressive': calculate_safe_leverage(p5_dd, safety_buffer=1.0)\n",
    "        }\n",
    "\n",
    "lev_df = pd.DataFrame(leverage_recs).T\n",
    "print(f\"Leverage Recommendations for {SYMBOL}:\")\n",
    "print(\"\\n(Based on rolling drawdown analysis)\")\n",
    "display(lev_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# Summary visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Extreme moves by time window\nax = axes[0]\nwindow_names = list(windows.keys())\nmax_moves = [percentile_stats[w]['max'] for w in window_names]\np99_moves = [percentile_stats[w]['p99'] for w in window_names]\n\nx = np.arange(len(window_names))\nwidth = 0.35\nax.bar(x - width/2, max_moves, width, label='Maximum Observed', color=COLORS['danger'])\nax.bar(x + width/2, p99_moves, width, label='99th Percentile', color=COLORS['warning'])\nax.set_xlabel('Time Window')\nax.set_ylabel('Absolute Return (%)')\nax.set_title('Extreme Price Movements')\nax.set_xticks(x)\nax.set_xticklabels(window_names)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\n# Right: Safe leverage by risk tolerance\nax = axes[1]\nif leverage_recs:\n    categories = ['Conservative', 'Moderate', 'Aggressive']\n    lev_cols = ['max_leverage_conservative', 'max_leverage_moderate', 'max_leverage_aggressive']\n    colors_lev = [COLORS['success'], COLORS['warning'], COLORS['danger']]\n    \n    x = np.arange(len(leverage_recs))\n    width = 0.25\n    \n    for i, (cat, col) in enumerate(zip(categories, lev_cols)):\n        values = [leverage_recs[w][col] for w in leverage_recs.keys()]\n        # Cap at 100x for visualization\n        values = [min(v, 100) for v in values]\n        ax.bar(x + i*width, values, width, label=cat, color=colors_lev[i])\n    \n    ax.set_xlabel('Reaction Time Window')\n    ax.set_ylabel('Maximum Leverage')\n    ax.set_title('Recommended Maximum Leverage')\n    ax.set_xticks(x + width)\n    ax.set_xticklabels(leverage_recs.keys())\n    ax.legend(title='Risk Tolerance')\n    ax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig(DATA_DIR / 'leverage_recommendations.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Oracle Deviation Analysis\n",
    "\n",
    "For multi-oracle aggregation (as proposed in `steps.md`), we need to understand:\n",
    "- How much prices typically deviate within a second\n",
    "- What threshold should trigger a deviation alert\n",
    "\n",
    "The intra-candle range (high - low) gives us insight into short-term price uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze intra-second price ranges for deviation threshold recommendations\n",
    "intra_range = returns_df['intra_range_pct'].dropna()\n",
    "\n",
    "print(\"Intra-Second Price Range Statistics:\")\n",
    "print(f\"  Mean: {intra_range.mean():.4f}%\")\n",
    "print(f\"  Median: {intra_range.median():.4f}%\")\n",
    "print(f\"  Std Dev: {intra_range.std():.4f}%\")\n",
    "print(f\"  90th percentile: {intra_range.quantile(0.90):.4f}%\")\n",
    "print(f\"  95th percentile: {intra_range.quantile(0.95):.4f}%\")\n",
    "print(f\"  99th percentile: {intra_range.quantile(0.99):.4f}%\")\n",
    "print(f\"  Maximum: {intra_range.max():.4f}%\")\n",
    "\n",
    "# Recommendation for oracle deviation threshold\n",
    "recommended_threshold = intra_range.quantile(0.99) * 2  # 2x the 99th percentile\n",
    "print(f\"\\nRecommended Oracle Deviation Threshold: {recommended_threshold:.4f}%\")\n",
    "print(\"(Based on 2x the 99th percentile of intra-second price range)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": "## 8. Conclusions and Recommendations\n\n### Key Findings\n\nBased on historical 1-second spot price data analysis:\n\n1. **Price Movement Distribution**: Most 1-second moves are tiny, but fat tails exist\n2. **Extreme Events**: The 99.99th percentile shows potential for significant moves\n3. **Rolling Drawdowns**: 5-minute windows can see substantial adverse moves\n\n### Recommendations\n\n| Setting | Conservative | Moderate | Aggressive |\n|---------|--------------|----------|------------|\n| Max Leverage | See above | See above | See above |\n| Oracle Deviation Threshold | 1% | 0.5% | 0.25% |\n| Liquidation Delay | 5 min | 1 min | 30 sec |\n\n### Next Steps\n\n1. **Fetch Full Year Data**: `poetry run python scripts/download_klines.py --symbol BTCUSDT --days 365`\n2. **Analyze Other Assets**: Add ETH, SOL, and altcoins\n3. **Compare Spot vs Perp**: Analyze deviations between prices\n4. **Backtest Liquidations**: Simulate liquidation engine on historical data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary statistics to CSV for reference\n",
    "summary = {\n",
    "    'symbol': SYMBOL,\n",
    "    'data_start': str(df.index.min()),\n",
    "    'data_end': str(df.index.max()),\n",
    "    'total_rows': len(df),\n",
    "    'return_1s_p99': percentile_stats['1s']['p99'],\n",
    "    'return_1m_p99': percentile_stats['1m']['p99'],\n",
    "    'return_5m_p99': percentile_stats['5m']['p99'],\n",
    "    'max_1s_move': percentile_stats['1s']['max'],\n",
    "    'max_5m_move': percentile_stats['5m']['max'],\n",
    "    'recommended_deviation_threshold': recommended_threshold\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv(DATA_DIR / f'{SYMBOL}_analysis_summary.csv', index=False)\n",
    "print(f\"Summary saved to {DATA_DIR / f'{SYMBOL}_analysis_summary.csv'}\")\n",
    "display(summary_df.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}